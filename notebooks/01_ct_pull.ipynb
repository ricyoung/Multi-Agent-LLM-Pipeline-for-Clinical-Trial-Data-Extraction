{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Trail Research\n",
    "\n",
    "## Clinical Trail Data Analysis - Richard Young\n",
    "\n",
    "### ryoung@unlv.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated 2025-01-012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review / Look at for ideas\n",
    "\n",
    "https://github.com/RyanWangZf/PyTrial\n",
    "\n",
    "https://stackoverflow.com/questions/78415818/how-to-get-full-results-with-clinicaltrials-gov-api-in-python\n",
    "\n",
    "\n",
    "some code based on https://github.com/jvfe/pytrials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime as dt\n",
    "\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "np.random.seed(10031975)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import *\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import openpyxl\n",
    "# import openai\n",
    "import re\n",
    "\n",
    "from urllib.parse import quote\n",
    "# import logging\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10\n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check directory for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subdirs = [\n",
    "    \"01_data_raw\"\n",
    "    # \"01_data_raw/interventions\",\n",
    "    # \"data_paper_tables\",\n",
    "    # \"data_results\",\n",
    "]\n",
    "\n",
    "for subdir in subdirs:\n",
    "    os.makedirs(subdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note sure I need this cell anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all functions for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Clinical Trials Calls / Functions\n",
    "\n",
    "def create_yearly_count_series(column_name):\n",
    "    yearly_counts = summary[column_name].dt.year.value_counts().sort_index()\n",
    "    # Adjusted to include the current research year\n",
    "    last_10_years = yearly_counts[(yearly_counts.index >= current_research_year - 10) & (yearly_counts.index <= current_research_year)]\n",
    "    return last_10_years.rename(column_name)\n",
    "\n",
    "# Function to save NCTid and year data for each column\n",
    "def save_nctid_year_data(column_name, file_name):\n",
    "    filtered_data = summary[summary[column_name].dt.year >= current_research_year - 10]\n",
    "    filtered_data['Year'] = filtered_data[column_name].dt.year\n",
    "    year_nctid_data = filtered_data[['Year', 'NCTid']]\n",
    "    file_path = os.path.join('data_results', file_name)\n",
    "    year_nctid_data.to_excel(file_path, index=False)\n",
    "    print(f\"{column_name} data saved at {file_path}\")\n",
    "\n",
    "def get_study_count(condition, search_area=\"condition\", page_size=1):\n",
    "    \"\"\"\n",
    "    Get the total count of studies for a given condition from ClinicalTrials.gov API.\n",
    "    \"\"\"\n",
    "    base_url = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "    encoded_condition = quote(condition)\n",
    "\n",
    "    search_area_params = {\n",
    "        \"condition\": \"query.cond\",\n",
    "        \"title\": \"query.term\",\n",
    "        \"intervention\": \"query.intr\",\n",
    "        \"outcome\": \"query.outc\",\n",
    "        \"sponsor\": \"query.spons\"\n",
    "    }\n",
    "\n",
    "    if search_area not in search_area_params:\n",
    "        print(f\"ERROR: Invalid search area: {search_area}\")\n",
    "        return None\n",
    "\n",
    "    params = {\n",
    "        search_area_params[search_area]: encoded_condition,\n",
    "        \"pageSize\": str(page_size),\n",
    "        \"countTotal\": \"true\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        total_count = data.get('totalCount', 0)\n",
    "\n",
    "        print(f\"INFO: URL: {response.url}\")\n",
    "        print(f\"INFO: Total count: {total_count}\")\n",
    "\n",
    "        return total_count\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"ERROR: An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def compare_study_counts(csv_path, api_study_count):\n",
    "    \"\"\"\n",
    "    Compare the count of unique NCTIds in the CSV file with the count from the API.\n",
    "    \"\"\"\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    unique_study_count_in_df = df['NCTId'].nunique()\n",
    "    print(f\"INFO: Unique studies in DataFrame: {unique_study_count_in_df}\")\n",
    "\n",
    "    if api_study_count is None:\n",
    "        print(\"ERROR: Failed to retrieve study count from API.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"INFO: Study count from ClinicalTrials API: {api_study_count}\")\n",
    "\n",
    "    # Compare counts\n",
    "    if unique_study_count_in_df == api_study_count:\n",
    "        print(\"INFO: The counts match.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"INFO: The counts do not match. DataFrame has {unique_study_count_in_df}, API reports {api_study_count}.\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = 'aged'\n",
    "# disease =  'amyotrophic lateral sclerosis'\n",
    "# disease = 'depression'\n",
    "# disease = 'diabetes'\n",
    "# disease = 'tacs'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the headers\n",
    "headers = [\n",
    "    \"Index\", \"NCTId\", \"LeadSponsorClass\", \"LeadSponsorName\", \"Condition\", \"OfficialTitle\",\n",
    "    \"BriefTitle\", \"Acronym\", \"StudyType\", \"InterventionType\", \"InterventionName\",\n",
    "    \"InterventionOtherName\", \"InterventionDescription\", \"Phase\", \"StudyFirstSubmitDate\",\n",
    "    \"LastUpdateSubmitDate\", \"CompletionDate\", \"OverallStatus\", \"BriefSummary\",\n",
    "    \"IsFDARegulatedDevice\", \"StartDate\", \"DetailedDescription\", \"ConditionMeshTerm\",\n",
    "    \"PrimaryOutcomeDescription\", \"SecondaryOutcomeDescription\", \"EnrollmentCount\",\n",
    "    \"EnrollmentType\", \"BaselineCategoryTitle\", \"BaselinePopulationDescription\",\n",
    "    \"BaselineTypeUnitsAnalyzed\", \"OtherOutcomeDescription\", \"EligibilityCriteria\",\n",
    "    \"StudyPopulation\", \"HealthyVolunteers\", \"ReferencePMID\", \"LocationCountry\",\n",
    "    \"PrimaryOutcomeTimeFrame\", \"BaselineMeasureTitle\", \"BaselineMeasureUnitOfMeasure\",\n",
    "    \"BaselineMeasurementValue\", \"GPT_summary\",\n",
    "]\n",
    "\n",
    "# Set your parameters\n",
    "csv_file_path = os.path.join('01_data_raw', f'01_{disease.lower()}_done.csv')\n",
    "\n",
    "# Get the total study count\n",
    "total_study_count = get_study_count(disease)\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(csv_file_path):\n",
    "    # Create the file and write the headers\n",
    "    with open(csv_file_path, 'w') as file:\n",
    "        file.write(','.join(headers) + '\\n')\n",
    "\n",
    "skip_next_cells = False\n",
    "\n",
    "# Ensure the total study count is correctly retrieved\n",
    "if total_study_count is not None:\n",
    "    # Save the API count to a DataFrame\n",
    "    api_count_df = pd.DataFrame({'Condition': [disease], 'API_Study_Count': [total_study_count]})\n",
    "    print(api_count_df.to_string(index=False))\n",
    "\n",
    "    # Compare the counts\n",
    "    counts_match = compare_study_counts(csv_file_path, total_study_count)\n",
    "\n",
    "    if counts_match is None:\n",
    "        print(\"ERROR: Comparison failed due to API error.\")\n",
    "        skip_next_cells = True  # Skip next cells because of API error\n",
    "    elif counts_match:\n",
    "        print(\"INFO: Skipping next cells as counts match.\")\n",
    "        skip_next_cells = True  # Skip next cells because counts match\n",
    "    else:\n",
    "        print(\"INFO: Proceeding with data update...\")\n",
    "        skip_next_cells = False  # Do not skip cells because counts do not match\n",
    "\n",
    "def compare_study_counts(csv_path, api_study_count):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'NCTId' not in df.columns:\n",
    "        print(\"ERROR: NCTId column not found in the CSV file.\")\n",
    "        return None\n",
    "    unique_study_count_in_df = df['NCTId'].nunique()\n",
    "    print(f\"INFO: Unique studies in DataFrame: {unique_study_count_in_df}\")\n",
    "    print(f\"INFO: Study count from ClinicalTrials API: {api_study_count}\")\n",
    "\n",
    "    if api_study_count is None:\n",
    "        return None\n",
    "\n",
    "    if unique_study_count_in_df == api_study_count:\n",
    "        print(\"INFO: The counts match.\")\n",
    "    else:\n",
    "        print(f\"INFO: The counts do not match. DataFrame has {unique_study_count_in_df}, API reports {api_study_count}.\")\n",
    "\n",
    "    return unique_study_count_in_df == api_study_count\n",
    "\n",
    "# Example usage\n",
    "print(f\"INFO: URL: https://clinicaltrials.gov/api/v2/studies?query.cond={disease}&pageSize=1&countTotal=true\")\n",
    "print(f\"INFO: Total count: {total_study_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_studies(search_terms, total_study_count, iter_size=987):\n",
    "    \"\"\"\n",
    "    Fetches study data in chunks from ClinicalTrials.gov API and processes it into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    search_terms (dict): Dictionary with keys as search areas and values as search terms.\n",
    "    total_study_count (int): Total number of studies to fetch.\n",
    "    iter_size (int, optional): Number of studies to fetch at a time. Default is 987.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the fetched study data.\n",
    "    \"\"\"\n",
    "    base_url = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "\n",
    "    search_area_params = {\n",
    "        \"condition\": \"query.cond\",\n",
    "        \"title\": \"query.term\",\n",
    "        \"intervention\": \"query.intr\",\n",
    "        \"outcome\": \"query.outc\",\n",
    "        \"sponsor\": \"query.spons\"\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"pageSize\": str(iter_size),\n",
    "        \"countTotal\": \"true\"\n",
    "    }\n",
    "\n",
    "    # Build search parameters based on search_terms\n",
    "    for area, term in search_terms.items():\n",
    "        if area in search_area_params:\n",
    "            params[search_area_params[area]] = term\n",
    "        else:\n",
    "            print(f\"ERROR: Invalid search area: {area}\")\n",
    "            return None\n",
    "\n",
    "    data_list = []  # List to store each chunk of data\n",
    "\n",
    "    while True:\n",
    "        # Print the current URL (for debugging purposes)\n",
    "        print(\"Fetching data from:\", base_url + '?' + '&'.join([f\"{k}={v}\" for k, v in params.items()]))\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            studies = data.get('studies', [])\n",
    "\n",
    "            for study in studies:\n",
    "                nctId = study['protocolSection']['identificationModule'].get('nctId', 'Unknown')\n",
    "                overallStatus = study['protocolSection']['statusModule'].get('overallStatus', 'Unknown')\n",
    "                startDate = study['protocolSection']['statusModule'].get('startDateStruct', {}).get('date', 'Unknown Date')\n",
    "                conditions = ', '.join(study['protocolSection']['conditionsModule'].get('conditions', ['No conditions listed']))\n",
    "                acronym = study['protocolSection']['identificationModule'].get('acronym', 'Unknown')\n",
    "\n",
    "                interventions_list = study['protocolSection'].get('armsInterventionsModule', {}).get('interventions', [])\n",
    "                interventions = ', '.join([intervention.get('interventionName', 'No intervention name listed') for intervention in interventions_list]) if interventions_list else \"No interventions listed\"\n",
    "\n",
    "                locations_list = study['protocolSection'].get('contactsLocationsModule', {}).get('locations', [])\n",
    "                locations = ', '.join([f\"{location.get('city', 'No City')} - {location.get('country', 'No Country')}\" for location in locations_list]) if locations_list else \"No locations listed\"\n",
    "\n",
    "                primaryCompletionDate = study['protocolSection']['statusModule'].get('primaryCompletionDateStruct', {}).get('date', 'Unknown Date')\n",
    "                studyFirstPostDate = study['protocolSection']['statusModule'].get('studyFirstPostDateStruct', {}).get('date', 'Unknown Date')\n",
    "                lastUpdatePostDate = study['protocolSection']['statusModule'].get('lastUpdatePostDateStruct', {}).get('date', 'Unknown Date')\n",
    "                studyType = study['protocolSection']['designModule'].get('studyType', 'Unknown')\n",
    "                phases = ', '.join(study['protocolSection']['designModule'].get('phases', ['Not Available']))\n",
    "\n",
    "                data_list.append({\n",
    "                    \"NCTId\": nctId,\n",
    "                    \"Acronym\": acronym,\n",
    "                    \"Overall Status\": overallStatus,\n",
    "                    \"Start Date\": startDate,\n",
    "                    \"Conditions\": conditions,\n",
    "                    \"Interventions\": interventions,\n",
    "                    \"Locations\": locations,\n",
    "                    \"Primary Completion Date\": primaryCompletionDate,\n",
    "                    \"Study First Post Date\": studyFirstPostDate,\n",
    "                    \"Last Update Post Date\": lastUpdatePostDate,\n",
    "                    \"Study Type\": studyType,\n",
    "                    \"Phases\": phases\n",
    "                })\n",
    "\n",
    "            if len(data_list) >= total_study_count:\n",
    "                break\n",
    "\n",
    "            nextPageToken = data.get('nextPageToken')\n",
    "            if nextPageToken:\n",
    "                params['pageToken'] = nextPageToken\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"ERROR: An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define 'search_terms' and 'total_study_count' if not already defined\n",
    "search_terms = {\n",
    "    \"condition\": disease.capitalize(),\n",
    "    \"title\": disease.capitalize(),\n",
    "}\n",
    "\n",
    "\n",
    "# Get the total number of studies for the disease\n",
    "total_study_count = get_study_count(disease)\n",
    "\n",
    "# Fetch new data from the API using the correct function\n",
    "df_new = fetch_studies(search_terms, total_study_count)\n",
    "\n",
    "# Load existing data\n",
    "csv_file_path = os.path.join('01_data_raw', f'01_{disease.lower()}_done.csv')\n",
    "df_existing = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Ensure column names are consistent between df_existing and df_new\n",
    "print(\"Columns in df_existing:\", df_existing.columns.tolist())\n",
    "print(\"Columns in df_new:\", df_new.columns.tolist())\n",
    "\n",
    "# Rename columns in df_new if necessary to match df_existing\n",
    "if 'NCT ID' in df_new.columns:\n",
    "    df_new.rename(columns={'NCT ID': 'NCTId'}, inplace=True)\n",
    "\n",
    "# Compare existing data with new data\n",
    "existing_nct_ids = set(df_existing['NCTId'])\n",
    "new_nct_ids = set(df_new['NCTId'])\n",
    "missing_nct_ids = new_nct_ids - existing_nct_ids\n",
    "\n",
    "# Get the missing data\n",
    "df_missing = df_new[df_new['NCTId'].isin(missing_nct_ids)]\n",
    "\n",
    "# Merge existing data with missing data\n",
    "df_combined = pd.concat([df_existing, df_missing], ignore_index=True)\n",
    "\n",
    "df_combined.drop_duplicates(subset='NCTId', inplace=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "output_csv_path = os.path.join('01_data_raw', f'01_{disease.lower()}_done.csv')\n",
    "df_combined.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"Data fetching and merging complete. Combined data saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"Number of records in df_existing: {len(df_existing)}\")\n",
    "print(f\"Number of records in df_new: {len(df_new)}\")\n",
    "print(f\"Number of missing records to add: {len(df_missing)}\")\n",
    "print(f\"Total records after merging: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few rows of df_existing:\")\n",
    "print(df_existing.head())\n",
    "\n",
    "print(\"First few rows of df_new:\")\n",
    "print(df_new.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Start Getting the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d, parent_key='', sep='_', prefixes_to_replace=None):\n",
    "    \"\"\"\n",
    "    Flatten a nested dictionary, optionally replacing specified prefixes in keys.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    if prefixes_to_replace is None:\n",
    "        prefixes_to_replace = {}\n",
    "\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "\n",
    "            # Replace specified prefixes\n",
    "            for old_prefix, new_prefix in prefixes_to_replace.items():\n",
    "                if new_key.startswith(old_prefix):\n",
    "                    new_key = new_prefix + new_key[len(old_prefix):]\n",
    "\n",
    "            if isinstance(v, dict):\n",
    "                items.extend(flatten_dict(v, new_key, sep=sep, prefixes_to_replace=prefixes_to_replace).items())\n",
    "            elif isinstance(v, list):\n",
    "                if v and all(isinstance(i, dict) for i in v):\n",
    "                    for idx, item in enumerate(v):\n",
    "                        items.extend(flatten_dict(item, f\"{new_key}{sep}{idx}\", sep=sep, prefixes_to_replace=prefixes_to_replace).items())\n",
    "                else:\n",
    "                    items.append((new_key, v))\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "    else:\n",
    "        items.append((parent_key, d))\n",
    "    return dict(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_study_details_batch(nct_ids, batch_size=50):\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    base_url = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "    data_list = []\n",
    "\n",
    "    for i in range(0, len(nct_ids), batch_size):\n",
    "        batch_nct_ids = nct_ids[i:i + batch_size]\n",
    "        nct_ids_str = ','.join(batch_nct_ids)\n",
    "        params = {\n",
    "            \"format\": \"json\",\n",
    "            \"pageSize\": len(batch_nct_ids)\n",
    "        }\n",
    "        from urllib.parse import urlencode\n",
    "        query_string = urlencode(params)\n",
    "        request_url = f\"{base_url}?filter.ids={nct_ids_str}&{query_string}\"\n",
    "        print(f\"Fetching data from: {request_url}\")\n",
    "\n",
    "        response = requests.get(request_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            studies = data.get('studies', [])\n",
    "\n",
    "            if not studies:\n",
    "                print(f\"No studies found for NCTIds: {batch_nct_ids}\")\n",
    "                continue\n",
    "\n",
    "            for study in studies:\n",
    "                # Extract relevant sections\n",
    "                protocol = study.get('protocolSection', {})\n",
    "                identificationModule = protocol.get('identificationModule', {})\n",
    "                statusModule = protocol.get('statusModule', {})\n",
    "                sponsorModule = protocol.get('sponsorCollaboratorsModule', {})\n",
    "                descriptionModule = protocol.get('descriptionModule', {})\n",
    "                conditionsModule = protocol.get('conditionsModule', {})\n",
    "                designModule = protocol.get('designModule', {})\n",
    "                armsInterventionsModule = protocol.get('armsInterventionsModule', {})\n",
    "                outcomesModule = protocol.get('outcomesModule', {})\n",
    "                eligibilityModule = protocol.get('eligibilityModule', {})\n",
    "                contactsModule = protocol.get('contactsLocationsModule', {})\n",
    "                referencesModule = protocol.get('referencesModule', {})\n",
    "\n",
    "                # Process LocationCountry field with error handling\n",
    "                locations_data = contactsModule.get('locations', [])\n",
    "                location_countries = []\n",
    "\n",
    "                try:\n",
    "                    if isinstance(locations_data, list):\n",
    "                        for loc in locations_data:\n",
    "                            if isinstance(loc, dict):\n",
    "                                facility = loc.get('facility', {})\n",
    "                                if isinstance(facility, dict):\n",
    "                                    country = facility.get('country', '')\n",
    "                                    if country:\n",
    "                                        location_countries.append(country)\n",
    "                    elif isinstance(locations_data, dict):\n",
    "                        facility = locations_data.get('facility', {})\n",
    "                        if isinstance(facility, dict):\n",
    "                            country = facility.get('country', '')\n",
    "                            if country:\n",
    "                                location_countries.append(country)\n",
    "                    elif isinstance(locations_data, str):\n",
    "                        location_countries.append(locations_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing locations for study {identificationModule.get('nctId', '')}: {e}\")\n",
    "                    location_countries = []\n",
    "\n",
    "                # Extract specific fields\n",
    "                study_data = {\n",
    "                    'NCTId': identificationModule.get('nctId', ''),\n",
    "                    'LeadSponsorClass': sponsorModule.get('leadSponsor', {}).get('class', ''),\n",
    "                    'LeadSponsorName': sponsorModule.get('leadSponsor', {}).get('name', ''),\n",
    "                    'Condition': ', '.join(conditionsModule.get('conditions', [])),\n",
    "                    'OfficialTitle': identificationModule.get('officialTitle', ''),\n",
    "                    'BriefTitle': identificationModule.get('briefTitle', ''),\n",
    "                    'Acronym': identificationModule.get('acronym', ''),\n",
    "                    'StudyType': designModule.get('studyType', ''),\n",
    "                    'InterventionType': ', '.join([intervention.get('interventionType', '') for intervention in armsInterventionsModule.get('interventions', [])]),\n",
    "                    'InterventionName': ', '.join([intervention.get('interventionName', intervention.get('name', '')) for intervention in armsInterventionsModule.get('interventions', [])]),\n",
    "                    'InterventionOtherName': ', '.join([', '.join(intervention.get('otherName', [])) for intervention in armsInterventionsModule.get('interventions', [])]),\n",
    "                    'InterventionDescription': ', '.join([intervention.get('description', '') for intervention in armsInterventionsModule.get('interventions', [])]),\n",
    "                    'Phase': ', '.join(designModule.get('phases', [])),\n",
    "                    'StudyFirstSubmitDate': statusModule.get('studyFirstSubmitDate', ''),\n",
    "                    'LastUpdateSubmitDate': statusModule.get('lastUpdateSubmitDate', ''),\n",
    "                    'CompletionDate': statusModule.get('completionDateStruct', {}).get('date', ''),\n",
    "                    'OverallStatus': statusModule.get('overallStatus', ''),\n",
    "                    'BriefSummary': descriptionModule.get('briefSummary', ''),\n",
    "                    'IsFDARegulatedDevice': protocol.get('oversightModule', {}).get('isFdaRegulatedDevice', ''),\n",
    "                    'StartDate': statusModule.get('startDateStruct', {}).get('date', ''),\n",
    "                    'DetailedDescription': descriptionModule.get('detailedDescription', ''),\n",
    "                    'ConditionMeshTerm': ', '.join([mesh.get('term', '') for mesh in protocol.get('derivedSection', {}).get('conditionBrowseModule', {}).get('meshes', [])]),\n",
    "                    'PrimaryOutcomeDescription': ', '.join([outcome.get('description', '') for outcome in outcomesModule.get('primaryOutcomes', [])]),\n",
    "                    'SecondaryOutcomeDescription': ', '.join([outcome.get('description', '') for outcome in outcomesModule.get('secondaryOutcomes', [])]),\n",
    "                    'EnrollmentCount': designModule.get('enrollmentInfo', {}).get('count', ''),\n",
    "                    'EnrollmentType': designModule.get('enrollmentInfo', {}).get('type', ''),\n",
    "                    'EligibilityCriteria': eligibilityModule.get('eligibilityCriteria', ''),\n",
    "                    'StudyPopulation': eligibilityModule.get('studyPopulation', ''),\n",
    "                    'HealthyVolunteers': eligibilityModule.get('healthyVolunteers', ''),\n",
    "                    'ReferencePMID': ', '.join([ref.get('pmid', '') for ref in referencesModule.get('references', []) if 'pmid' in ref]),\n",
    "                    'LocationCountry': ', '.join(location_countries),\n",
    "                    'PrimaryOutcomeTimeFrame': ', '.join([outcome.get('timeFrame', '') for outcome in outcomesModule.get('primaryOutcomes', [])]),\n",
    "                    # Add other fields as required\n",
    "                }\n",
    "\n",
    "                # Append to data list\n",
    "                data_list.append(study_data)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "            print(f\"Response content: {response.text}\")\n",
    "            continue\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare existing data with new data\n",
    "existing_nct_ids = set(df_existing['NCTId'])\n",
    "new_nct_ids = set(df_new['NCTId'])\n",
    "missing_nct_ids = list(new_nct_ids - existing_nct_ids)\n",
    "\n",
    "print(f\"Number of missing NCTIds to fetch: {len(missing_nct_ids)}\")\n",
    "\n",
    "# Fetch missing data using the updated function\n",
    "df_missing_details = fetch_study_details_batch(missing_nct_ids, batch_size=25)\n",
    "\n",
    "# Verify the data fetched\n",
    "print(\"First few rows of df_missing_details:\")\n",
    "print(df_missing_details.head())\n",
    "\n",
    "# Check if df_missing_details is empty\n",
    "if df_missing_details.empty:\n",
    "    print(\"No data was fetched for the missing NCTIds.\")\n",
    "else:\n",
    "    # Combine with existing data\n",
    "    df_combined = pd.concat([df_existing, df_missing_details], ignore_index=True)\n",
    "    df_combined.drop_duplicates(subset='NCTId', inplace=True)\n",
    "\n",
    "    # Save the combined data\n",
    "    output_csv_path_csv = os.path.join('01_data_raw', f'01_{disease.lower()}_done.csv')\n",
    "    output_excel_path = os.path.join('01_data_raw', f'01_{disease.lower()}_done.xlsx')\n",
    "\n",
    "    df_combined.to_csv(output_csv_path_csv, index=False)\n",
    "    df_combined.to_excel(output_excel_path, index=False)\n",
    "\n",
    "    print(\"Data fetching and merging complete. Combined data saved to:\", output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'GPT_summary' column exists, if not, add it\n",
    "if 'GPT_summary' not in df_combined.columns:\n",
    "    df_combined.insert(3, 'GPT_summary', '')  # Insert as the 4th column (index 3)\n",
    "\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df_combined.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_combined is your DataFrame\n",
    "df_combined['IsFDARegulatedDevice'] = df_combined['IsFDARegulatedDevice'].replace('', np.nan)\n",
    "\n",
    "# Ensure 'IsFDARegulatedDevice' column contains only strings or NaN\n",
    "df_combined['IsFDARegulatedDevice'] = df_combined['IsFDARegulatedDevice'].astype(str).replace('nan', np.nan)\n",
    "\n",
    "# Replace empty strings in 'HealthyVolunteers' with NaN\n",
    "df_combined['HealthyVolunteers'] = df_combined['HealthyVolunteers'].replace('', np.nan)\n",
    "\n",
    "# Ensure 'HealthyVolunteers' column contains only strings or NaN\n",
    "df_combined['HealthyVolunteers'] = df_combined['HealthyVolunteers'].astype(str).replace('nan', np.nan)\n",
    "\n",
    "# Convert 'EnrollmentCount' to numeric, coercing errors to NaN\n",
    "df_combined['EnrollmentCount'] = pd.to_numeric(df_combined['EnrollmentCount'], errors='coerce')\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = os.path.join('01_data_raw', f'01_{disease.lower()}_done.csv')\n",
    "df_combined.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Save to Parquet\n",
    "parquet_file_path = os.path.join('01_data_raw', f'01_{disease.lower()}.parquet')\n",
    "df_combined.to_parquet(parquet_file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {csv_file_path} and {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "resp = requests.post('https://textbelt.com/text', {\n",
    "\n",
    "  'phone': '9163802941',\n",
    "\n",
    "  'message': 'PD Pipe Line Part 1 Done',\n",
    "\n",
    "  'key': '138adc496234ca311154757db147f552afa8ba83FfrCKJ36kTJNXq65nlsvvF4Pu',\n",
    "\n",
    "})\n",
    "\n",
    "print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infection point - both new and old pulls should be the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Do some data cleaning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PD_research_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}